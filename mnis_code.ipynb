{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnis code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtSJ/NvkzRZ5rchGUSnULw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhoanmartinezz/-masking-of-categorical-variables/blob/master/mnis_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKZ5WyYYKPB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1.instalar las dependencias y entornos de gpu\n",
        "\n",
        "#2.importar las dependencias necesarias para el proyecto\n",
        "\n",
        "#3.limpiar los datos \n",
        "\n",
        "#4.cargar dataset \n",
        "\n",
        "#5.normalizando el dataset \n",
        "\n",
        "#6.redimensionandolo a la forma que espera la capa totalmente conectada, entran 784 pixeles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRB86xXlJQK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "cdea4174-f2ef-4a06-b14c-06a4b90c431b"
      },
      "source": [
        "#1.instalar los dependecias requeridas\n",
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.30.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (49.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-TyGfJyKbrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2.importa las dependencias \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqAJNpkpufkf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6325998f-b907-4ee7-eacd-8ea5cc482b15"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0vYhjKbLfkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3.pre procesado de datos\n",
        "#cargar el dataset\n",
        "\n",
        "# X_train => lleva las 60.000 imagenes bidimensionales distribuidos los pixeles en filas y columnas\n",
        "# X_test => se lleva las 10.000 imagenes bidimensionales que tienen los pixeles de las imagenes \n",
        "# y_train :. y_test => es un vector de clases significando las clases del conjunto de entrenamiento, las \n",
        "  #10 posibles vategorias de clasificacion de cada una de las imagenes de 60.000 valores \n",
        "# sin hacer la redimension se tendrian 784 pixeles para cada imagen\n",
        "# vienen en filas y columnas \n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#normalizar \n",
        "#convertir a un rango entre 0 y 1 por ejemplo, lo que hace es la red neuronal converja\n",
        "#mas rapidamente y entienda mucho mejor y evitar el overfitting y entrenara mas rapido\n",
        "\n",
        "#dividir cada pixel de cada imagen entre el valor mas grande que pueda tomar, esta es\n",
        "#una operacion tensorial, lo quiero aplicar a todos los pixeles de cada imagen del dataset\n",
        "\n",
        "#X_train vendria siendo un tensor tridimensional 60.000 filas * 28 columnas *28 datos de profundidad\n",
        "#la primera dimension nos da el indice de la imagen que estamos tratando img 1,2,3,4\n",
        "#las otras dos dimensiones serian las dos dimensiones del array conteniendo los \n",
        "#pixeles de las imagenes en fila y columna, la fila selecciona la foto y columna y\n",
        "#profundidad seria para leer los pixeles bidimensionales de la imagen\n",
        "\n",
        "x_train = x_train/255.0\n",
        "x_test = x_test/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejX6xWmXfzCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cda42cbf-6b4a-4e5b-ace2-de03f0072010"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBEDmwKIf4Ve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48a46ccb-ff44-4782-cc7a-cd16278ae53d"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZcw14ehfgbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#redimensionar el dataset \n",
        "#convertir todo de formato matriz a formato vector, documentacion reshape tensorflow\n",
        "#60000 vectores fila conteniendo cada uno de ellos 784 valores, lo que se hace es\n",
        "#una transformacion a modo de la segunda dimension para aplanar todos los pixeles \n",
        "#de cada imagen en un unico vector fila haciendo uso de la funcion re-shape\n",
        "# -1 toma todas las filas para que tengan un formato de 28*28\n",
        "x_train = x_train.reshape(-1, 28*28)\n",
        "x_test = x_test.reshape(-1, 28*28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2McCw47iWgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f78be5d3-addd-46ed-db84-c15a60133a39"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2_AjLOV6XQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8beb944-8f8e-4336-caab-91a4e44dc49d"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJABfcjIjFXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#4. construir la red neuronal artificial\n",
        "#definir el modelo \n",
        "#capas una detras de la otra todas conectadas\n",
        "#modelo que vamo a usar para ir añadiendo capa tras capa\n",
        "# la informacion de la arquitectura de la red neuronal \n",
        "model = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZuC-wB9ZHVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#5.capa totalmente conectada o capa densa donde entran las imagenes \n",
        "\n",
        "#hiperparametros de la capa\n",
        "\n",
        "# ===> numero de unidades/neuronas = 128\n",
        "# ===> funcion de activacion = ReLU \n",
        "# ===> input shape = (784, ) = tamaño de la capa de entrada, 1 fila y columna vacia\n",
        "# y = x * w + b ===> ReLU(y) ===> output\n",
        "model.add(tf.keras.layers.Dense(units=60, activation='relu', input_shape=(784, )))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbmMflVSbTbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#6. agregar capa de dropout\n",
        "\n",
        "#droptout es una tecnica de regularizacion donde aleatoriamente se asignan a ciertas\n",
        "#neuronas de la red neuronal no se activan, de este modo mientras se entrena,\n",
        "#estas neuronas no actualizaran sus valores (los pesos o weigths W)\n",
        "\n",
        "#Al tener cierto porcentaje de neuronas sin actualizar, el proceso de entrenamiento\n",
        "#toma mas tiempo pero por contra tenemos menos posibilidad de sufrir de overfitting\n",
        "model.add(tf.keras.layers.Dropout(0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RinkFxKBcOEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#7. agregar la capa de salida o de dropout(capa de salida)\n",
        "#unidades de nuero de clases de salida \n",
        "#devuelve 10 probabilidades \n",
        "#funcion de activacion softmax(devuelve la probabilidad del la clase a la que pertenece)\n",
        "#devuelve la probabilida de que una de las imagenes pertenezca a una de las 10 categorias \n",
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wllIf4Z6dqRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#8. compilar el modelo\n",
        "#compilar es conectar la arquitectura a un optimizador y elegir la funcion de perdida\n",
        "#que el optimizador va a intentar minimizar para actualizar los pesos de la red\n",
        "#neuronal utilizando la tecnica del gradiente descendente estocastico cuando haga\n",
        "#propagacion hacia atras de los errores cometidos en el proceso de clasificacion \n",
        "\n",
        "#------------------------------------------------------------------------------+\n",
        "#           Sintaxis para clasificacion de mas de 2 categorias                 |\n",
        "#------------------------------------------------------------------------------+\n",
        "\n",
        "#optimizer ===> De los mejores optimizadores esta el Adam, puede ser un gradiente descendente o\n",
        "#gradiente descendente estocastico o una mezcla de ambas \n",
        "#es uno de los mejores optimizadores de gradiente descendente estocastico que existe\n",
        "#opcion por defecto si sabe cual usar\n",
        "\n",
        "#loss ===> funcion de perdidas que basicamente es como va a calcular el error cometido\n",
        "#en la prediccion, calcular el error cometido entre la prediccion del modelo y la \n",
        "#categoria de verdad a la que realmente estaba etiquetada como tal la imagen\n",
        "\n",
        "#metrics ===> medir como de correcta o incorrecta es la clasificacion de la red neuronal\n",
        "#accuracy = Numero correcto de predicciones / numero total de observaciones (60000)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGR7TILLhKNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7c6cc5d2-3504-48ac-f800-5fa0e26ee897"
      },
      "source": [
        "#------------------------------------------------------------------------------+\n",
        "#          9. Funcion summary para un detalle de lo que hay                    |\n",
        "#------------------------------------------------------------------------------+\n",
        "\n",
        "#la funcion summary sobre el modelo se utiliza para ver un resumen de todo lo que\n",
        "#hace falta o de todos los parametros que van a ser calculados duranre la fase de\n",
        "#entrenamiento de la red neuronal \n",
        "#muestra la cantidad de capas de la arquitectura\n",
        "\n",
        "# _________________________________________________________________\n",
        "# Layer (type)          Output Shape              Param #   \n",
        "# =================================================================\n",
        "# dense (Dense)         (None, 128 neuronas)     100480 pesos (weights)  128+785*28 conexiones + 1 adicional termino independiente=====> primera capa \n",
        "# _________________________________________________________________\n",
        "# dropout (Dropout)     (None, 128 neuronas)        0   pesos (weights) no calcula parametro, duerme aleatoriamente las neuronas =====> segunda capa \n",
        "# _________________________________________________________________\n",
        "# dense_1 (Dense)       (None, 10 neuronas)       1290 pesos (weights) 128 capa anterior * 10 + 10 de termino independiente =====> tercera capa\n",
        "# =================================================================\n",
        "\n",
        "#Param son los parametros que el algoritmo va a necesitar calcular utilizando la\n",
        "#tecnica del gradiente descendente estocastico y la propagacion hacia atras\n",
        "\n",
        "# =================================================================\n",
        "#todos los parametros son entrenables, no hay ningun hyperpamaetro que el algoritmo\n",
        "#no pueda entrenar\n",
        "\n",
        "# Total params: 101,770\n",
        "# Trainable params: 101,770\n",
        "# Non-trainable params: 0\n",
        "# _________________________________________________________________\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_44 (Dense)             (None, 60)                47100     \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 60)                0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 10)                610       \n",
            "=================================================================\n",
            "Total params: 47,710\n",
            "Trainable params: 47,710\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNxU7ExRhyS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "9f1b801e-d4da-446c-8aee-6bac8fb50d81"
      },
      "source": [
        "#------------------------------------------------------------------------------+\n",
        "#                          10. Entrenar el modelo                              |\n",
        "#------------------------------------------------------------------------------+\n",
        "\n",
        "#pasa sobre el conjunto total de imagenes de X_train 5 veces y se van a obtener\n",
        "#unas predicciones de a que categoria pertenece cada una de las imagenes y se van\n",
        "#a constrastar contra el y_train \n",
        "#se va a aplicar la metrica de cross entropy con funcion de perdida y de precision\n",
        "#con metrica de que tan buena es la precision del algoritmo y a partir de ahi el algoritmo\n",
        "#ira corrigiendo con propagacion hacia atras con el optimizador de adam a travez\n",
        "#de localizar donde estan los posibles errores sobre todo el conjunto de entrenmaiento\n",
        "\n",
        "#hay un parametro batch size indica cuanto tiene que recordar y cuanto olvidar de\n",
        "#una iteracion a la siguiente para que e descendiente estocastico no sea solo en una \n",
        "#direccion \n",
        "\n",
        "#con batch size no calcularia los pesos hasta pasar de 10 en 10 si el bacth size es 10\n",
        "\n",
        "model.fit(x_train, y_train, epochs=7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3397 - sparse_categorical_accuracy: 0.8748\n",
            "Epoch 2/7\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3297 - sparse_categorical_accuracy: 0.8787\n",
            "Epoch 3/7\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3214 - sparse_categorical_accuracy: 0.8818\n",
            "Epoch 4/7\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3168 - sparse_categorical_accuracy: 0.8822\n",
            "Epoch 5/7\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3130 - sparse_categorical_accuracy: 0.8850\n",
            "Epoch 6/7\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3066 - sparse_categorical_accuracy: 0.8856\n",
            "Epoch 7/7\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3016 - sparse_categorical_accuracy: 0.8885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8708360cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OVmsWRHlIci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ae9484c-6231-4b0a-f4e8-2681b1d52386"
      },
      "source": [
        "#------------------------------------------------------------------------------+\n",
        "#                          11. evaluacion y prediccion del modelo              |\n",
        "#------------------------------------------------------------------------------+\n",
        "#devuelve la perdida del conjunto de testing y la precision\n",
        "#           test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "#la primera es la perdida frente al conjunto de testing sparse categorical cross entrnopy\n",
        "#accuracy es que porcentaje del total ha sido predecido correctamente \n",
        "\n",
        "#model.evaluate(x_test,  y_test, verbose=2)\n",
        "#si se comporta mal es porque hay presencia del overfitting\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3450 - sparse_categorical_accuracy: 0.8784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPJ-aAXFttZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#------------------------------------------------------------------------------+\n",
        "#                     12. guardar la arquitectura de la red neuronal           |\n",
        "#------------------------------------------------------------------------------+\n",
        "model_json = model.to_json()\n",
        "with open(\"fashion_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nu9YyKA7OEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#------------------------------------------------------------------------------+\n",
        "#                     12. guardar los pesos de la red neuronal                 |\n",
        "#------------------------------------------------------------------------------+\n",
        "model.save_weights(\"fashion_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMSZD8X-7Vw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}